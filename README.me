ğŸ§± Visual ETL Pipeline Flow
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSV Files   â”‚
â”‚ data_sources â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Extract    â”‚
â”‚  (Python)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transform   â”‚
â”‚  (Pandas)    â”‚
â”‚  - Cleaning  â”‚
â”‚  - Enriching â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Load     â”‚
â”‚ PostgreSQL   â”‚
â”‚ SQLAlchemy   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Logging &  â”‚
â”‚   Testing    â”‚
â”‚  Pytest      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ Project Structure
etl_project/
â”‚
â”œâ”€â”€ extract/                 # Extract raw CSV data
â”‚   â””â”€â”€ extract_orders.py
â”‚
â”œâ”€â”€ transform/               # Data cleaning & transformation
â”‚   â””â”€â”€ transform_orders.py
â”‚
â”œâ”€â”€ load/                    # Load data into PostgreSQL
â”‚   â””â”€â”€ load_to_postgres.py
â”‚
â”œâ”€â”€ tests/                   # Automated ETL tests
â”‚   â””â”€â”€ test_etl_pipeline.py
â”‚
â”œâ”€â”€ data_sources/            # Source CSVs (empty on GitHub)
â”œâ”€â”€ raw_data/                # Extracted raw files
â”‚
â”œâ”€â”€ run_pipeline.py          # ETL orchestration script
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

âš™ï¸ Technologies Used

Python â€“ Core ETL logic

Pandas â€“ Data transformation and validation

PostgreSQL â€“ Target data warehouse

SQLAlchemy â€“ Database connectivity & ORM

Pytest â€“ Automated ETL testing

Docker (optional) â€“ Containerized database setup

Git / GitHub â€“ Version control

âœ¨ Key Features

âœ… Incremental loading to prevent duplicate records

âœ… Idempotent ETL runs (safe to re-run pipeline)

âœ… Modular architecture (extract, transform, load)

âœ… Automated tests for extract & transform stages

âœ… Detailed logging for observability and debugging

âœ… Scheduling support (cron / Windows Task Scheduler)

âœ… Resume-ready project structure

ğŸ§± Target Database Schema (PostgreSQL)
orders (
  order_id         INTEGER PRIMARY KEY,
  customer_id      INTEGER,
  order_date       DATE,
  amount           NUMERIC,
  tax              NUMERIC,
  amount_category  TEXT
);

ğŸš€ ETL Workflow

Extract

Reads raw CSV files from data_sources/

Copies raw data to raw_data/ for traceability

Transform

Cleans invalid or missing values

Computes derived fields (tax, amount category)

Ensures data quality rules

Load

Inserts only new records into PostgreSQL

Prevents duplicates via incremental logic

Logging

All stages log execution details to etl_log.txt

Testing

Pytest validates extraction and transformation outputs

ğŸƒ Running the Pipeline
Manual Execution
python run_pipeline.py

Automated Scheduling
Linux / WSL (Cron)
crontab -e
0 10 * * * /usr/bin/python3 /home/youruser/etl_project/run_pipeline.py >> /home/youruser/etl_project/etl_log.txt 2>&1

Windows Task Scheduler

Program: Full path to python.exe

Arguments: run_pipeline.py

Start in: etl_project

Logs written to etl_log.txt

âœ… Testing

Run automated tests:

pytest tests/

Tests Include

âœ” Extracted CSV file existence

âœ” Required transformed columns

âœ” No negative transaction amounts

ğŸ“‚ Logging

All ETL stages log timestamps and messages to etl_log.txt

Useful for:

Monitoring pipeline health

Debugging failures

Verifying successful runs

ğŸ¯ Project Goals

Build a reusable, production-style ETL pipeline

Apply data engineering best practices

Create a strong portfolio project for data engineering roles

ğŸ’¡ Notes

Empty directories are preserved using .gitkeep

.gitignore excludes:

Logs

Virtual environments

Cache files

Sensitive credentials

â­ Why This Project Matters

This project demonstrates:

Practical ETL design (not just scripts)

Incremental and idempotent data loading

Testing, logging, and orchestration

Skills directly applicable to Data Engineer / Analytics Engineer roles