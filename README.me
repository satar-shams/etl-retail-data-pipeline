üß± Visual ETL Pipeline Flow
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CSV Files   ‚îÇ
‚îÇ data_sources ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Extract    ‚îÇ
‚îÇ  (Python)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Transform   ‚îÇ
‚îÇ  (Pandas)    ‚îÇ
‚îÇ  - Cleaning  ‚îÇ
‚îÇ  - Enriching ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Load     ‚îÇ
‚îÇ PostgreSQL   ‚îÇ
‚îÇ SQLAlchemy   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Logging &  ‚îÇ
‚îÇ   Testing    ‚îÇ
‚îÇ  Pytest      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üìÅ Project Structure
etl_project/
‚îÇ
‚îú‚îÄ‚îÄ extract/                 # Extract raw CSV data
‚îÇ   ‚îî‚îÄ‚îÄ extract_orders.py
‚îÇ
‚îú‚îÄ‚îÄ transform/               # Data cleaning & transformation
‚îÇ   ‚îî‚îÄ‚îÄ transform_orders.py
‚îÇ
‚îú‚îÄ‚îÄ load/                    # Load data into PostgreSQL
‚îÇ   ‚îî‚îÄ‚îÄ load_to_postgres.py
‚îÇ
‚îú‚îÄ‚îÄ tests/                   # Automated ETL tests
‚îÇ   ‚îî‚îÄ‚îÄ test_etl_pipeline.py
‚îÇ
‚îú‚îÄ‚îÄ data_sources/            # Source CSVs (empty on GitHub)
‚îú‚îÄ‚îÄ raw_data/                # Extracted raw files
‚îÇ
‚îú‚îÄ‚îÄ run_pipeline.py          # ETL orchestration script
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .gitignore

‚öôÔ∏è Technologies Used

Python ‚Äì Core ETL logic

Pandas ‚Äì Data transformation and validation

PostgreSQL ‚Äì Target data warehouse

SQLAlchemy ‚Äì Database connectivity & ORM

Pytest ‚Äì Automated ETL testing

Docker (optional) ‚Äì Containerized database setup

Git / GitHub ‚Äì Version control

‚ú® Key Features

‚úÖ Incremental loading to prevent duplicate records

‚úÖ Idempotent ETL runs (safe to re-run pipeline)

‚úÖ Modular architecture (extract, transform, load)

‚úÖ Automated tests for extract & transform stages

‚úÖ Detailed logging for observability and debugging

‚úÖ Scheduling support (cron / Windows Task Scheduler)

‚úÖ Resume-ready project structure

üß± Target Database Schema (PostgreSQL)
orders (
  order_id         INTEGER PRIMARY KEY,
  customer_id      INTEGER,
  order_date       DATE,
  amount           NUMERIC,
  tax              NUMERIC,
  amount_category  TEXT
);

üöÄ ETL Workflow

Extract

Reads raw CSV files from data_sources/

Copies raw data to raw_data/ for traceability

Transform

Cleans invalid or missing values

Computes derived fields (tax, amount category)

Ensures data quality rules

Load

Inserts only new records into PostgreSQL

Prevents duplicates via incremental logic

Logging

All stages log execution details to etl_log.txt

Testing

Pytest validates extraction and transformation outputs

üèÉ Running the Pipeline
Manual Execution
python run_pipeline.py

Automated Scheduling
Linux / WSL (Cron)
crontab -e
0 10 * * * /usr/bin/python3 /home/youruser/etl_project/run_pipeline.py >> /home/youruser/etl_project/etl_log.txt 2>&1

Windows Task Scheduler

Program: Full path to python.exe

Arguments: run_pipeline.py

Start in: etl_project

Logs written to etl_log.txt

‚úÖ Testing

Run automated tests:

pytest tests/

Tests Include

‚úî Extracted CSV file existence

‚úî Required transformed columns

‚úî No negative transaction amounts

üìÇ Logging

All ETL stages log timestamps and messages to etl_log.txt

Useful for:

Monitoring pipeline health

Debugging failures

Verifying successful runs

üéØ Project Goals

Build a reusable, production-style ETL pipeline

Apply data engineering best practices

Create a strong portfolio project for data engineering roles

üí° Notes

Empty directories are preserved using .gitkeep

.gitignore excludes:

Logs

Virtual environments

Cache files

Sensitive credentials

‚≠ê Why This Project Matters

This project demonstrates:

Practical ETL design (not just scripts)

Incremental and idempotent data loading

Testing, logging, and orchestration

Skills directly applicable to Data Engineer / Analytics Engineer roles


## üõ†Ô∏è Monitoring & Logging

This ETL project includes a **robust monitoring and logging system** to track the execution of each pipeline step, detect errors, and monitor data quality.

### 1. Logging Implementation
- All scripts (`extract`, `transform`, `load`) use a **centralized logger** defined in `logger_config.py`.
- Logs are written to `logs/etl_log.txt`.
- The logging format includes:
Timestamp | Level | Module | Message

Example:
2026-02-05 19:12:16 | INFO | main | Total rows after transform: 1000


### 2. Log Levels
- **INFO**: Successful execution of each step, file paths, and row counts.
- **WARNING**: Data quality issues such as:
- Null values in important columns
- Negative values in `amount`
- Duplicate rows
- **ERROR**: Fatal issues such as missing files or database connection failures.

### 3. Pipeline Run Monitoring
- The main pipeline (`run_pipeline.py`) logs:
- Step start and end markers (`START EXTRACT`, `END EXTRACT`, etc.)
- Duration of each step
- Python executable and working directory
- Standard output and errors from each script

- Each run is separated by a **divider line and a blank line** for clarity in multi-run logs.

### 4. Benefits
- Easy to monitor pipeline execution in real-time or from `etl_log.txt`.
- Provides **automatic alerts** for data quality issues without interrupting the pipeline.
- Ideal for scheduled runs (cron jobs) or production monitoring.
- Enables **quick debugging** in case of failures.



üîê Secrets Management with Vault

To securely manage sensitive data (like database credentials), this ETL project uses HashiCorp Vault.

1Ô∏è‚É£ Why Vault?

Keeps credentials (username, password, host, port, database) out of the code.

Supports secure storage, versioning, and dynamic secrets.

Allows CI/CD pipelines to retrieve secrets securely.

2Ô∏è‚É£ Vault Setup (Development)

Run Vault in Docker:

docker run --cap-add=IPC_LOCK -e VAULT_DEV_ROOT_TOKEN_ID=root -e VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200 -p 8200:8200 --name vault-dev hashicorp/vault:latest


Start Vault if stopped:

docker start vault-dev


Enable KV-v2 secrets engine for ETL:

docker exec -it vault-dev sh -c "export VAULT_ADDR='http://127.0.0.1:8200' && export VAULT_TOKEN='root' && vault secrets enable -path=etl kv-v2"


Store PostgreSQL credentials in Vault:

docker exec -it vault-dev sh -c "
export VAULT_ADDR='http://127.0.0.1:8200' &&
export VAULT_TOKEN='root' &&
vault kv put etl/postgres username='postgres' password='1234' host='localhost' dbname='data_engineer_project' port='5432'
"


Retrieve credentials (test):

docker exec -it vault-dev sh -c "export VAULT_ADDR='http://127.0.0.1:8200' && export VAULT_TOKEN='root' && vault kv get etl/postgres"

3Ô∏è‚É£ Usage in ETL Scripts

The ETL code retrieves secrets using a Vault client:

from vault_client import get_secret

secrets = get_secret("etl/postgres")
db_url = (
    f"postgresql://{secrets['username']}:{secrets['password']}"
    f"@{secrets['host']}:{secrets['port']}/{secrets['dbname']}"
)


No credentials are stored in code or git repositories.

4Ô∏è‚É£ Notes for Production

Do not use development mode (VAULT_DEV_ROOT_TOKEN_ID=root) in production.

In production, use a secure Vault cluster and CI/CD secrets injection.

Vault allows rotation of credentials without modifying code.

‚úÖ This section ensures anyone can reproduce your Vault setup locally and understand how secrets are handled in the ETL pipeline.